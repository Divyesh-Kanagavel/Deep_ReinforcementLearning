{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tabular Q learning on Frozen Lake environment.\n",
        "algorithm : Start with an empty table \\\n",
        "Obtain (s,a,r,s') from the environment \\\n",
        "Make a bellman update: \\\n",
        "Q(s,a) -> (1-$\\alpha$) * Q(s,a) + $\\alpha$ * (r + $\\gamma$ * max(Q(s',a'))) \\\n",
        "check convergene condition  , else repeat step 2"
      ],
      "metadata": {
        "id": "rIal21Q171gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9u3ZZCe9mnl",
        "outputId": "f53da86d-3e71-4fac-be8f-d1aeededa028"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import gymnasium as gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.2\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lir4suzS8fU7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()[0]\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def sample_env(self):\n",
        "        action = self.env.action_space.sample()\n",
        "        old_state = self.state\n",
        "        new_state, reward, is_done, _ ,_= self.env.step(action)\n",
        "        self.state = self.env.reset()[0] if is_done else new_state\n",
        "        return (old_state, action, reward, new_state)\n",
        "\n",
        "    def best_value_and_action(self, state):\n",
        "        best_value, best_action = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_value, best_action\n",
        "\n",
        "    def value_update(self, s, a, r, next_s):\n",
        "        best_v, _ = self.best_value_and_action(next_s)\n",
        "        new_val = r + GAMMA * best_v\n",
        "        old_val = self.values[(s, a)]\n",
        "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()[0]\n",
        "        while True:\n",
        "            _, action = self.best_value_and_action(state)\n",
        "            new_state, reward, is_done, _,_ = env.step(action)\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OlQz4yaW9ai6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test environment\n",
        "test_env = gym.make(ENV_NAME)\n",
        "agent = Agent()\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "  iter_no += 1\n",
        "  s, a,r, s_dash = agent.sample_env()\n",
        "  agent.value_update(s,a,r,s_dash)\n",
        "  reward = 0.0\n",
        "  for _ in range(TEST_EPISODES):\n",
        "    reward += agent.play_episode(test_env)\n",
        "  reward /= TEST_EPISODES\n",
        "\n",
        "  if reward > best_reward:\n",
        "    best_reward = reward\n",
        "    print(f\"the best reward is {best_reward:.2f}\")\n",
        "\n",
        "  if reward > 0.80:\n",
        "    print(\"solved\")\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0guLbUTD9oJ",
        "outputId": "42e24bb9-626e-477e-9031-376dc6c5b1e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best reward is 0.25\n",
            "the best reward is 0.35\n",
            "the best reward is 0.40\n",
            "the best reward is 0.45\n",
            "the best reward is 0.80\n",
            "the best reward is 0.85\n",
            "solved\n"
          ]
        }
      ]
    }
  ]
}