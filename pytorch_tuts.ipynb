{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFDsXzW5GX0q"
      },
      "source": [
        "torch - a DL library for manipulation of tensors [multi-dimensional arrays]. \\\n",
        "supports 13 different types - float32, float16, bfloat16(higher exponent), float64\n",
        "complex : 32,64,128 bits, int : int8, uint8, int16, int32, int64 and bool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clLPQA3UGX0r"
      },
      "source": [
        "Tensors of different types are represented by different classes - torch.FloatTensor (for float32), torch.LongTensor(int64), torch.ByteTensor(uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF0GJupDGX0s",
        "outputId": "786fab3b-08ac-45f1-f1fe-d71871375261"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[       nan, 1.4013e-45],\n",
              "        [1.3593e-43, 0.0000e+00],\n",
              "        [4.8354e-19, 0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "a = torch.FloatTensor(3,2) # calling the constructor\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "0lxubH7vGX0t"
      },
      "outputs": [],
      "source": [
        "a = torch.zeros(3,2) # torch.FloatTensor(3,2) initializes with zeros but in the previous versions, it kept the tensor uninitialized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHjb9ScFGX0t",
        "outputId": "224590ec-9588-4069-e9f7-c2a3e6afe070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "#alternative approach\n",
        "a = torch.FloatTensor(3,2)\n",
        "a.zero_()\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR5Sc4kiGX0u"
      },
      "source": [
        "Two types of operations on tensors : inplace and functional \\\n",
        "Inplace operations have underscore appended to their name and operate on the tensor's content. The functional equivalent creates a copy. \\\n",
        "Inplace - more efficient and does not require extra memory but might lead to hidden bugs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJp3m3GYGX0u",
        "outputId": "730f249e-42f4-444f-99f2-4c05a65ed58a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "# tensor from python iterable like list, tuple\n",
        "\n",
        "a = torch.FloatTensor([[1,2],[3,4],[5,6]])\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7qQtCyEGX0u",
        "outputId": "5dbfc0cb-a87c-41d4-cc86-8c010d3f6929"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, 2), dtype('float64'))"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "n = np.zeros(shape = (3,2))\n",
        "n.shape , n.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "U0FC2_P9GX0v"
      },
      "outputs": [],
      "source": [
        "b = torch.tensor(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-4YfcNnGX0v",
        "outputId": "aa9dd775-1eb1-4fa2-ec9a-3d3e6dcac06f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 2]), torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "b.shape, b.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmzteEsRGX0v"
      },
      "source": [
        "Usually in deep learning, float64 is too much memory overhead. float32 or float16 is enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSp6l0oAGX0w"
      },
      "source": [
        "to convert from numpy array to torch tensor, torch.from_numpy was used but is now deprecated and torch.tensor() is encouraged with torch datatypes available as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q605Fl5sGX0w",
        "outputId": "f26e8914-3899-4c62-ca01-6d9d04cae17f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2) float64\n"
          ]
        }
      ],
      "source": [
        "n = np.zeros(shape=(3,2))\n",
        "print(n.shape, n.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XN3DaQ_GX0w",
        "outputId": "85c04ae7-8e4e-49ab-c862-b2ceea4630a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor(n, dtype=torch.float32)\n",
        "print(t.shape, t.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ6g8LkZGX0w",
        "outputId": "1ba0ad33-60a9-44ea-e8ab-0a0bdb075f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "#Scalar tensor - Now, zero-dimensional tensors are natively supported and returned by the appropriate functions\n",
        "a = torch.tensor([1,2,3])\n",
        "s = a.sum()\n",
        "print(s.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWdZPmhvGX0x",
        "outputId": "de043fff-fa98-4e9e-dbc6-ef36b4f45a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(s.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spDlO31nGX0x"
      },
      "source": [
        "GPU tensors:\n",
        "Pytorch supports CUDA GPUs. it has two versions - CPU and GPU.where to process the tensors depends on the tensor configuration. GPU tensors reside in the torch.cuda class instead of the torch package. So, the tensor is torch.cuda.FloatTensor instead of torch.FloatTensor. \\\n",
        "Under the hood, there is no CPU, GPU. there is a backend, which is an abstract computation device with memory. it could be CUDA, CPU or Apple Metal performance Shader given by mps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip1KeBB_GX0x",
        "outputId": "337e55f4-8fd6-4b68-e64b-236c31727b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "a = torch.Tensor([1,2,3,4])\n",
        "print(a.shape, a.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiKtA2AXGX0y",
        "outputId": "ac9f6a9e-ee25-4ee0-d06a-4fb8a74b450d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "c = a.to('cpu') # tensor copied to Apple's MPS\n",
        "c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxyxEAHHGX0y"
      },
      "source": [
        "device = 'mps:0' refers to the fact that the computation device in use for tensor c is mps and it uses the first card. if there are multiple cards, we could have mps:1 as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQdFu1k6GX0y",
        "outputId": "e62a233e-2247-4f03-dc5c-af0cbf745840"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 3., 4., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "a+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB-P7y6DGX0y",
        "outputId": "c2b0ba55-de8e-44f9-9a16-1a681e5c13e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 3., 4., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "c+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rmxqws5GX0z",
        "outputId": "288c33be-d217-41be-8ad3-452f1f985d13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "c.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTIT1fYlGX0z"
      },
      "source": [
        "Gradient calculation methods :\n",
        "Static graph method : define your calculations in advance and cannot be changed later. graph is optimized by the dl library like tensorflow/theano and many other DL toolkits\n",
        "Dynamic graph method : As you apply transformations on the data, the dl library will keep track of the computations and when requested will compute the gradients , accumulating the gradients of the network parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X29tntOJGX0z"
      },
      "source": [
        "From version 2.0, pytorch has torch.compile() method which speeds up pytorch code by using JIT (just in time) compiling into optimized kernels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM-jgxGxGX00"
      },
      "source": [
        "Gradients :\n",
        "Pytorch has useful fields related to gradient computation of tensors like \\\n",
        "grad : A property that holds a tensor of the same shape containing computed gradients.\n",
        "is_leaf : true if created by user, false if part of function transformation\n",
        "requires_grads : Equals true if the tensor requires gradients to be calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "B9Mduk7qGX00"
      },
      "outputs": [],
      "source": [
        "v1 = torch.tensor([1.,2.,3.,4.], requires_grad=True)\n",
        "v2 = torch.tensor([5.,6.,7.,8.]) # by default requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "VhEEDXHIGX00"
      },
      "outputs": [],
      "source": [
        "v3 = v1 + v2\n",
        "v_res = (v3 * 2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAfOEpHiGX00",
        "outputId": "498cc313-a22b-486a-d3cf-4895b5f380f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True, False, False)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "v1.is_leaf, v2.is_leaf, v3.is_leaf, v_res.is_leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFxtLwYnGX00",
        "outputId": "07a93695-0aad-40fe-a81f-d912975c5c70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, False, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "v1.requires_grad, v2.requires_grad, v3.requires_grad, v_res.requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bto-TX6GX00"
      },
      "source": [
        "gradients are preserved only for the leaf nodes for memory reasons. So, if we want to gradients to be preserved for non-leaf nodes, retain_grad() method is to be called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "CxH08lBgGX00"
      },
      "outputs": [],
      "source": [
        "v_res.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMwQakkXGX01",
        "outputId": "595aaee1-7a5b-4516-b8d1-96e0bbde0147"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "v1.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "GzAlRMF0GX01"
      },
      "outputs": [],
      "source": [
        "v2.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-EUIOd0GX01",
        "outputId": "57ac3b12-243a-42fd-f67a-9c53b6118585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-7ceff49f5bd2>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  v3.grad\n"
          ]
        }
      ],
      "source": [
        "v3.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7AXyXVRGX01"
      },
      "source": [
        "torch.nn has many useful building blocks used in neural networks.\n",
        "it is designed to be callable. create an instance of the class and then pass input to it just like function is called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nam3LXu_GX01",
        "outputId": "0d0cabcf-0c31-46d6-d986-618848bc787b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.8457, -1.9337,  0.0750,  0.5338, -1.7690], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "l = nn.Linear(2,5)\n",
        "v = torch.FloatTensor([1.,2.])\n",
        "l(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GIg0z91GX01"
      },
      "source": [
        "Some useful methods of nn.Module Base class from which torch.nn packages inherit and to create custom NN blocks, we use this class :\n",
        "parameters() - return all module parameters(weights) \\\n",
        "to(device) - transfer module parameters to cpu or gpu \\\n",
        "zero_grad() - zero out the module gradients \\\n",
        "state_dict() - returns the dictionary with all module parameters and is useful for model serialization\n",
        "load_state_dict() - loads the module with dictionary of model parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFhP2e8NGX01",
        "outputId": "e86ca4eb-00d5-4bbf-92b9-44d32215096a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
              "  (5): Dropout(p=0.3, inplace=False)\n",
              "  (6): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "#example\n",
        "s= nn.Sequential(\n",
        "    nn.Linear(2,5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5,20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20,10),\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOfRKgJ7GX02",
        "outputId": "fe8882ee-434f-4c95-dcac-d56f1771be49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1293, 0.0451, 0.0826, 0.1143, 0.0882, 0.1394, 0.1208, 0.1057, 0.1143,\n",
              "         0.0603]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "s(torch.FloatTensor([[1.,2.]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "6fsZte_aGX02"
      },
      "outputs": [],
      "source": [
        "# creating custom nn layer from nn.Module\n",
        "class OurModule(nn.Module):\n",
        "    def __init__(self, num_inputs, num_classes, dropout = 0.3):\n",
        "        super(OurModule, self).__init__()\n",
        "        self.pipe = nn.Sequential(\n",
        "            nn.Linear(num_inputs, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(5,20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, num_classes),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.pipe(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rp-TzTYGX02"
      },
      "source": [
        "To write our custom layer, we have to inherit from the base nn.Module and override the forward API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2KYIxyWGX02",
        "outputId": "bbfee585-f543-40d5-f4a2-9a6a56b744a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OurModule(\n",
            "  (pipe): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net = OurModule(num_inputs=2, num_classes=3)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "9J1XGw8eGX02"
      },
      "outputs": [],
      "source": [
        "v = torch.FloatTensor([[2.,3.]])\n",
        "out = net(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8-0JO51GX02",
        "outputId": "2d9aa181-b8b4-477e-abe8-85c019564d55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2355, 0.1142, 0.6503]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehLSnTF6GX02",
        "outputId": "54ac128f-ed8b-4b4f-ad5a-ee91a8f47056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mps availability :  False\n"
          ]
        }
      ],
      "source": [
        "print(\"mps availability : \", torch.mps.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1deUrbNGX02",
        "outputId": "e0772d5d-e75a-402d-b7f9-16e1e2672aca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2355, 0.1142, 0.6503]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "out.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "c71WaypfGX03"
      },
      "outputs": [],
      "source": [
        "#Tensorboard for visualization of neural network metrics\n",
        "import math\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "\n",
        "funs = {\"sin\":math.sin, \"cos\":math.cos, \"tan\":math.tan}\n",
        "writer = SummaryWriter()\n",
        "for angle in range(-360,360):\n",
        "    angle_rad = angle * math.pi / 180\n",
        "    for name, fun in funs.items():\n",
        "        val = fun(angle_rad)\n",
        "        writer.add_scalar(name, val, angle)\n",
        "writer.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEH_ygOdGX03"
      },
      "source": [
        "Generative Adversial networks on Atari images :\n",
        "GANs are neural networks which are used to generate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tgm54TL5GX03"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 64"
      ],
      "metadata": {
        "id": "A7eGxvS0HYyw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputWrapper(gym.ObservationWrapper):\n",
        "  def __init__(self,*args):\n",
        "    super(InputWrapper,self).__init__(*args)\n",
        "    old_space = self.observation_space\n",
        "    assert isinstance(old_space, gym.spaces.Box)\n",
        "    self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high), dtype=np.float32)\n",
        "  def observation(self, observation):\n",
        "    new_obs = cv2.resize(observation,(IMAGE_SIZE,IMAGE_SIZE))\n",
        "    # transpose from (w,h,c) -> (c,w,h)\n",
        "    new_obs = np.moveaxis(new_obs,2,0)\n",
        "    return new_obs.astype(np.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "RsM6OUF4Gg4b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "metadata": {
        "id": "76NTFJpRNgE3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LATENT_VECTOR_SIZE = 100\n",
        "DISCR_FILTERS = 64\n",
        "GENER_FILTERS = 64"
      ],
      "metadata": {
        "id": "Ab4dnTfXNnk0"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let us now code up the generator and discriminator networks"
      ],
      "metadata": {
        "id": "DM4SePurKK4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disciminator : outputs a probability value that gives a measure of fakeness of the image produced by generator network\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,input_shape):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.pipe = nn.Sequential(\n",
        "        nn.Conv2d(input_shape[0], DISCR_FILTERS, 4, 2, 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(DISCR_FILTERS, DISCR_FILTERS*2, 4, 2, 1),\n",
        "        nn.BatchNorm2d(DISCR_FILTERS*2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(DISCR_FILTERS*2, DISCR_FILTERS*4, 4, 2, 1),\n",
        "        nn.BatchNorm2d(DISCR_FILTERS*4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(DISCR_FILTERS*4, DISCR_FILTERS*8, 4, 2, 1),\n",
        "        nn.BatchNorm2d(DISCR_FILTERS*8),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(DISCR_FILTERS*8, 1, 4, 1, 0),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    conv_out =  self.pipe(x)\n",
        "    return conv_out.view(-1,1).squeeze(dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Smw3174cHo7Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, output_shape):\n",
        "    super(Generator,self).__init__()\n",
        "    self.pipe = nn.Sequential(\n",
        "        nn.ConvTranspose2d(LATENT_VECTOR_SIZE,GENER_FILTERS*8, 4,1,0),\n",
        "        nn.BatchNorm2d(GENER_FILTERS*8),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(GENER_FILTERS*8,GENER_FILTERS*4, 4,2,1),\n",
        "        nn.BatchNorm2d(GENER_FILTERS*4),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(GENER_FILTERS*4,GENER_FILTERS*2,4,2,1),\n",
        "        nn.BatchNorm2d(GENER_FILTERS*2),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(GENER_FILTERS*2,GENER_FILTERS,4,2,1),\n",
        "        nn.BatchNorm2d(GENER_FILTERS),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(GENER_FILTERS,output_shape[0], 4,2,1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.pipe(x)"
      ],
      "metadata": {
        "id": "Fbc1mDenULId"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_batches(envs, batch_size):\n",
        "  batch = [e.reset()[0] for e in envs] # extract the environment after resetting\n",
        "  env_gen = iter(lambda : random.choice(envs),None) # returns an env from list of envs till None is returned\n",
        "  while True:\n",
        "    e = next(env_gen)\n",
        "    action = e.action_space.sample() # random action, we are more interested in extracting images from the envs\n",
        "    obs, reward, is_done,is_trunc,_ = e.step(action)\n",
        "    if np.mean(obs) > 0.01 : # to fix the glitch in the env\n",
        "        batch.append(obs)\n",
        "    if len(batch) == batch_size:\n",
        "      batch_np = np.array(batch, dtype=np.float32)\n",
        "      yield torch.tensor(batch_np * 2.0 / 255. - 1.)\n",
        "      batch.clear()\n",
        "    if is_done or is_trunc:\n",
        "      e.reset()\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GohM2MVYfp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "YRIScPOhY5UB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.envs.registry.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqImt__hapu8",
        "outputId": "ed9d70f1-768f-4648-b405-a13c5c5ce444"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0', 'Adventure-v0', 'AdventureDeterministic-v0', 'AdventureNoFrameskip-v0', 'Adventure-v4', 'AdventureDeterministic-v4', 'AdventureNoFrameskip-v4', 'Adventure-ram-v0', 'Adventure-ramDeterministic-v0', 'Adventure-ramNoFrameskip-v0', 'Adventure-ram-v4', 'Adventure-ramDeterministic-v4', 'Adventure-ramNoFrameskip-v4', 'AirRaid-v0', 'AirRaidDeterministic-v0', 'AirRaidNoFrameskip-v0', 'AirRaid-v4', 'AirRaidDeterministic-v4', 'AirRaidNoFrameskip-v4', 'AirRaid-ram-v0', 'AirRaid-ramDeterministic-v0', 'AirRaid-ramNoFrameskip-v0', 'AirRaid-ram-v4', 'AirRaid-ramDeterministic-v4', 'AirRaid-ramNoFrameskip-v4', 'Alien-v0', 'AlienDeterministic-v0', 'AlienNoFrameskip-v0', 'Alien-v4', 'AlienDeterministic-v4', 'AlienNoFrameskip-v4', 'Alien-ram-v0', 'Alien-ramDeterministic-v0', 'Alien-ramNoFrameskip-v0', 'Alien-ram-v4', 'Alien-ramDeterministic-v4', 'Alien-ramNoFrameskip-v4', 'Amidar-v0', 'AmidarDeterministic-v0', 'AmidarNoFrameskip-v0', 'Amidar-v4', 'AmidarDeterministic-v4', 'AmidarNoFrameskip-v4', 'Amidar-ram-v0', 'Amidar-ramDeterministic-v0', 'Amidar-ramNoFrameskip-v0', 'Amidar-ram-v4', 'Amidar-ramDeterministic-v4', 'Amidar-ramNoFrameskip-v4', 'Assault-v0', 'AssaultDeterministic-v0', 'AssaultNoFrameskip-v0', 'Assault-v4', 'AssaultDeterministic-v4', 'AssaultNoFrameskip-v4', 'Assault-ram-v0', 'Assault-ramDeterministic-v0', 'Assault-ramNoFrameskip-v0', 'Assault-ram-v4', 'Assault-ramDeterministic-v4', 'Assault-ramNoFrameskip-v4', 'Asterix-v0', 'AsterixDeterministic-v0', 'AsterixNoFrameskip-v0', 'Asterix-v4', 'AsterixDeterministic-v4', 'AsterixNoFrameskip-v4', 'Asterix-ram-v0', 'Asterix-ramDeterministic-v0', 'Asterix-ramNoFrameskip-v0', 'Asterix-ram-v4', 'Asterix-ramDeterministic-v4', 'Asterix-ramNoFrameskip-v4', 'Asteroids-v0', 'AsteroidsDeterministic-v0', 'AsteroidsNoFrameskip-v0', 'Asteroids-v4', 'AsteroidsDeterministic-v4', 'AsteroidsNoFrameskip-v4', 'Asteroids-ram-v0', 'Asteroids-ramDeterministic-v0', 'Asteroids-ramNoFrameskip-v0', 'Asteroids-ram-v4', 'Asteroids-ramDeterministic-v4', 'Asteroids-ramNoFrameskip-v4', 'Atlantis-v0', 'AtlantisDeterministic-v0', 'AtlantisNoFrameskip-v0', 'Atlantis-v4', 'AtlantisDeterministic-v4', 'AtlantisNoFrameskip-v4', 'Atlantis-ram-v0', 'Atlantis-ramDeterministic-v0', 'Atlantis-ramNoFrameskip-v0', 'Atlantis-ram-v4', 'Atlantis-ramDeterministic-v4', 'Atlantis-ramNoFrameskip-v4', 'BankHeist-v0', 'BankHeistDeterministic-v0', 'BankHeistNoFrameskip-v0', 'BankHeist-v4', 'BankHeistDeterministic-v4', 'BankHeistNoFrameskip-v4', 'BankHeist-ram-v0', 'BankHeist-ramDeterministic-v0', 'BankHeist-ramNoFrameskip-v0', 'BankHeist-ram-v4', 'BankHeist-ramDeterministic-v4', 'BankHeist-ramNoFrameskip-v4', 'BattleZone-v0', 'BattleZoneDeterministic-v0', 'BattleZoneNoFrameskip-v0', 'BattleZone-v4', 'BattleZoneDeterministic-v4', 'BattleZoneNoFrameskip-v4', 'BattleZone-ram-v0', 'BattleZone-ramDeterministic-v0', 'BattleZone-ramNoFrameskip-v0', 'BattleZone-ram-v4', 'BattleZone-ramDeterministic-v4', 'BattleZone-ramNoFrameskip-v4', 'BeamRider-v0', 'BeamRiderDeterministic-v0', 'BeamRiderNoFrameskip-v0', 'BeamRider-v4', 'BeamRiderDeterministic-v4', 'BeamRiderNoFrameskip-v4', 'BeamRider-ram-v0', 'BeamRider-ramDeterministic-v0', 'BeamRider-ramNoFrameskip-v0', 'BeamRider-ram-v4', 'BeamRider-ramDeterministic-v4', 'BeamRider-ramNoFrameskip-v4', 'Berzerk-v0', 'BerzerkDeterministic-v0', 'BerzerkNoFrameskip-v0', 'Berzerk-v4', 'BerzerkDeterministic-v4', 'BerzerkNoFrameskip-v4', 'Berzerk-ram-v0', 'Berzerk-ramDeterministic-v0', 'Berzerk-ramNoFrameskip-v0', 'Berzerk-ram-v4', 'Berzerk-ramDeterministic-v4', 'Berzerk-ramNoFrameskip-v4', 'Bowling-v0', 'BowlingDeterministic-v0', 'BowlingNoFrameskip-v0', 'Bowling-v4', 'BowlingDeterministic-v4', 'BowlingNoFrameskip-v4', 'Bowling-ram-v0', 'Bowling-ramDeterministic-v0', 'Bowling-ramNoFrameskip-v0', 'Bowling-ram-v4', 'Bowling-ramDeterministic-v4', 'Bowling-ramNoFrameskip-v4', 'Boxing-v0', 'BoxingDeterministic-v0', 'BoxingNoFrameskip-v0', 'Boxing-v4', 'BoxingDeterministic-v4', 'BoxingNoFrameskip-v4', 'Boxing-ram-v0', 'Boxing-ramDeterministic-v0', 'Boxing-ramNoFrameskip-v0', 'Boxing-ram-v4', 'Boxing-ramDeterministic-v4', 'Boxing-ramNoFrameskip-v4', 'Breakout-v0', 'BreakoutDeterministic-v0', 'BreakoutNoFrameskip-v0', 'Breakout-v4', 'BreakoutDeterministic-v4', 'BreakoutNoFrameskip-v4', 'Breakout-ram-v0', 'Breakout-ramDeterministic-v0', 'Breakout-ramNoFrameskip-v0', 'Breakout-ram-v4', 'Breakout-ramDeterministic-v4', 'Breakout-ramNoFrameskip-v4', 'Carnival-v0', 'CarnivalDeterministic-v0', 'CarnivalNoFrameskip-v0', 'Carnival-v4', 'CarnivalDeterministic-v4', 'CarnivalNoFrameskip-v4', 'Carnival-ram-v0', 'Carnival-ramDeterministic-v0', 'Carnival-ramNoFrameskip-v0', 'Carnival-ram-v4', 'Carnival-ramDeterministic-v4', 'Carnival-ramNoFrameskip-v4', 'Centipede-v0', 'CentipedeDeterministic-v0', 'CentipedeNoFrameskip-v0', 'Centipede-v4', 'CentipedeDeterministic-v4', 'CentipedeNoFrameskip-v4', 'Centipede-ram-v0', 'Centipede-ramDeterministic-v0', 'Centipede-ramNoFrameskip-v0', 'Centipede-ram-v4', 'Centipede-ramDeterministic-v4', 'Centipede-ramNoFrameskip-v4', 'ChopperCommand-v0', 'ChopperCommandDeterministic-v0', 'ChopperCommandNoFrameskip-v0', 'ChopperCommand-v4', 'ChopperCommandDeterministic-v4', 'ChopperCommandNoFrameskip-v4', 'ChopperCommand-ram-v0', 'ChopperCommand-ramDeterministic-v0', 'ChopperCommand-ramNoFrameskip-v0', 'ChopperCommand-ram-v4', 'ChopperCommand-ramDeterministic-v4', 'ChopperCommand-ramNoFrameskip-v4', 'CrazyClimber-v0', 'CrazyClimberDeterministic-v0', 'CrazyClimberNoFrameskip-v0', 'CrazyClimber-v4', 'CrazyClimberDeterministic-v4', 'CrazyClimberNoFrameskip-v4', 'CrazyClimber-ram-v0', 'CrazyClimber-ramDeterministic-v0', 'CrazyClimber-ramNoFrameskip-v0', 'CrazyClimber-ram-v4', 'CrazyClimber-ramDeterministic-v4', 'CrazyClimber-ramNoFrameskip-v4', 'Defender-v0', 'DefenderDeterministic-v0', 'DefenderNoFrameskip-v0', 'Defender-v4', 'DefenderDeterministic-v4', 'DefenderNoFrameskip-v4', 'Defender-ram-v0', 'Defender-ramDeterministic-v0', 'Defender-ramNoFrameskip-v0', 'Defender-ram-v4', 'Defender-ramDeterministic-v4', 'Defender-ramNoFrameskip-v4', 'DemonAttack-v0', 'DemonAttackDeterministic-v0', 'DemonAttackNoFrameskip-v0', 'DemonAttack-v4', 'DemonAttackDeterministic-v4', 'DemonAttackNoFrameskip-v4', 'DemonAttack-ram-v0', 'DemonAttack-ramDeterministic-v0', 'DemonAttack-ramNoFrameskip-v0', 'DemonAttack-ram-v4', 'DemonAttack-ramDeterministic-v4', 'DemonAttack-ramNoFrameskip-v4', 'DoubleDunk-v0', 'DoubleDunkDeterministic-v0', 'DoubleDunkNoFrameskip-v0', 'DoubleDunk-v4', 'DoubleDunkDeterministic-v4', 'DoubleDunkNoFrameskip-v4', 'DoubleDunk-ram-v0', 'DoubleDunk-ramDeterministic-v0', 'DoubleDunk-ramNoFrameskip-v0', 'DoubleDunk-ram-v4', 'DoubleDunk-ramDeterministic-v4', 'DoubleDunk-ramNoFrameskip-v4', 'ElevatorAction-v0', 'ElevatorActionDeterministic-v0', 'ElevatorActionNoFrameskip-v0', 'ElevatorAction-v4', 'ElevatorActionDeterministic-v4', 'ElevatorActionNoFrameskip-v4', 'ElevatorAction-ram-v0', 'ElevatorAction-ramDeterministic-v0', 'ElevatorAction-ramNoFrameskip-v0', 'ElevatorAction-ram-v4', 'ElevatorAction-ramDeterministic-v4', 'ElevatorAction-ramNoFrameskip-v4', 'Enduro-v0', 'EnduroDeterministic-v0', 'EnduroNoFrameskip-v0', 'Enduro-v4', 'EnduroDeterministic-v4', 'EnduroNoFrameskip-v4', 'Enduro-ram-v0', 'Enduro-ramDeterministic-v0', 'Enduro-ramNoFrameskip-v0', 'Enduro-ram-v4', 'Enduro-ramDeterministic-v4', 'Enduro-ramNoFrameskip-v4', 'FishingDerby-v0', 'FishingDerbyDeterministic-v0', 'FishingDerbyNoFrameskip-v0', 'FishingDerby-v4', 'FishingDerbyDeterministic-v4', 'FishingDerbyNoFrameskip-v4', 'FishingDerby-ram-v0', 'FishingDerby-ramDeterministic-v0', 'FishingDerby-ramNoFrameskip-v0', 'FishingDerby-ram-v4', 'FishingDerby-ramDeterministic-v4', 'FishingDerby-ramNoFrameskip-v4', 'Freeway-v0', 'FreewayDeterministic-v0', 'FreewayNoFrameskip-v0', 'Freeway-v4', 'FreewayDeterministic-v4', 'FreewayNoFrameskip-v4', 'Freeway-ram-v0', 'Freeway-ramDeterministic-v0', 'Freeway-ramNoFrameskip-v0', 'Freeway-ram-v4', 'Freeway-ramDeterministic-v4', 'Freeway-ramNoFrameskip-v4', 'Frostbite-v0', 'FrostbiteDeterministic-v0', 'FrostbiteNoFrameskip-v0', 'Frostbite-v4', 'FrostbiteDeterministic-v4', 'FrostbiteNoFrameskip-v4', 'Frostbite-ram-v0', 'Frostbite-ramDeterministic-v0', 'Frostbite-ramNoFrameskip-v0', 'Frostbite-ram-v4', 'Frostbite-ramDeterministic-v4', 'Frostbite-ramNoFrameskip-v4', 'Gopher-v0', 'GopherDeterministic-v0', 'GopherNoFrameskip-v0', 'Gopher-v4', 'GopherDeterministic-v4', 'GopherNoFrameskip-v4', 'Gopher-ram-v0', 'Gopher-ramDeterministic-v0', 'Gopher-ramNoFrameskip-v0', 'Gopher-ram-v4', 'Gopher-ramDeterministic-v4', 'Gopher-ramNoFrameskip-v4', 'Gravitar-v0', 'GravitarDeterministic-v0', 'GravitarNoFrameskip-v0', 'Gravitar-v4', 'GravitarDeterministic-v4', 'GravitarNoFrameskip-v4', 'Gravitar-ram-v0', 'Gravitar-ramDeterministic-v0', 'Gravitar-ramNoFrameskip-v0', 'Gravitar-ram-v4', 'Gravitar-ramDeterministic-v4', 'Gravitar-ramNoFrameskip-v4', 'Hero-v0', 'HeroDeterministic-v0', 'HeroNoFrameskip-v0', 'Hero-v4', 'HeroDeterministic-v4', 'HeroNoFrameskip-v4', 'Hero-ram-v0', 'Hero-ramDeterministic-v0', 'Hero-ramNoFrameskip-v0', 'Hero-ram-v4', 'Hero-ramDeterministic-v4', 'Hero-ramNoFrameskip-v4', 'IceHockey-v0', 'IceHockeyDeterministic-v0', 'IceHockeyNoFrameskip-v0', 'IceHockey-v4', 'IceHockeyDeterministic-v4', 'IceHockeyNoFrameskip-v4', 'IceHockey-ram-v0', 'IceHockey-ramDeterministic-v0', 'IceHockey-ramNoFrameskip-v0', 'IceHockey-ram-v4', 'IceHockey-ramDeterministic-v4', 'IceHockey-ramNoFrameskip-v4', 'Jamesbond-v0', 'JamesbondDeterministic-v0', 'JamesbondNoFrameskip-v0', 'Jamesbond-v4', 'JamesbondDeterministic-v4', 'JamesbondNoFrameskip-v4', 'Jamesbond-ram-v0', 'Jamesbond-ramDeterministic-v0', 'Jamesbond-ramNoFrameskip-v0', 'Jamesbond-ram-v4', 'Jamesbond-ramDeterministic-v4', 'Jamesbond-ramNoFrameskip-v4', 'JourneyEscape-v0', 'JourneyEscapeDeterministic-v0', 'JourneyEscapeNoFrameskip-v0', 'JourneyEscape-v4', 'JourneyEscapeDeterministic-v4', 'JourneyEscapeNoFrameskip-v4', 'JourneyEscape-ram-v0', 'JourneyEscape-ramDeterministic-v0', 'JourneyEscape-ramNoFrameskip-v0', 'JourneyEscape-ram-v4', 'JourneyEscape-ramDeterministic-v4', 'JourneyEscape-ramNoFrameskip-v4', 'Kangaroo-v0', 'KangarooDeterministic-v0', 'KangarooNoFrameskip-v0', 'Kangaroo-v4', 'KangarooDeterministic-v4', 'KangarooNoFrameskip-v4', 'Kangaroo-ram-v0', 'Kangaroo-ramDeterministic-v0', 'Kangaroo-ramNoFrameskip-v0', 'Kangaroo-ram-v4', 'Kangaroo-ramDeterministic-v4', 'Kangaroo-ramNoFrameskip-v4', 'Krull-v0', 'KrullDeterministic-v0', 'KrullNoFrameskip-v0', 'Krull-v4', 'KrullDeterministic-v4', 'KrullNoFrameskip-v4', 'Krull-ram-v0', 'Krull-ramDeterministic-v0', 'Krull-ramNoFrameskip-v0', 'Krull-ram-v4', 'Krull-ramDeterministic-v4', 'Krull-ramNoFrameskip-v4', 'KungFuMaster-v0', 'KungFuMasterDeterministic-v0', 'KungFuMasterNoFrameskip-v0', 'KungFuMaster-v4', 'KungFuMasterDeterministic-v4', 'KungFuMasterNoFrameskip-v4', 'KungFuMaster-ram-v0', 'KungFuMaster-ramDeterministic-v0', 'KungFuMaster-ramNoFrameskip-v0', 'KungFuMaster-ram-v4', 'KungFuMaster-ramDeterministic-v4', 'KungFuMaster-ramNoFrameskip-v4', 'MontezumaRevenge-v0', 'MontezumaRevengeDeterministic-v0', 'MontezumaRevengeNoFrameskip-v0', 'MontezumaRevenge-v4', 'MontezumaRevengeDeterministic-v4', 'MontezumaRevengeNoFrameskip-v4', 'MontezumaRevenge-ram-v0', 'MontezumaRevenge-ramDeterministic-v0', 'MontezumaRevenge-ramNoFrameskip-v0', 'MontezumaRevenge-ram-v4', 'MontezumaRevenge-ramDeterministic-v4', 'MontezumaRevenge-ramNoFrameskip-v4', 'MsPacman-v0', 'MsPacmanDeterministic-v0', 'MsPacmanNoFrameskip-v0', 'MsPacman-v4', 'MsPacmanDeterministic-v4', 'MsPacmanNoFrameskip-v4', 'MsPacman-ram-v0', 'MsPacman-ramDeterministic-v0', 'MsPacman-ramNoFrameskip-v0', 'MsPacman-ram-v4', 'MsPacman-ramDeterministic-v4', 'MsPacman-ramNoFrameskip-v4', 'NameThisGame-v0', 'NameThisGameDeterministic-v0', 'NameThisGameNoFrameskip-v0', 'NameThisGame-v4', 'NameThisGameDeterministic-v4', 'NameThisGameNoFrameskip-v4', 'NameThisGame-ram-v0', 'NameThisGame-ramDeterministic-v0', 'NameThisGame-ramNoFrameskip-v0', 'NameThisGame-ram-v4', 'NameThisGame-ramDeterministic-v4', 'NameThisGame-ramNoFrameskip-v4', 'Phoenix-v0', 'PhoenixDeterministic-v0', 'PhoenixNoFrameskip-v0', 'Phoenix-v4', 'PhoenixDeterministic-v4', 'PhoenixNoFrameskip-v4', 'Phoenix-ram-v0', 'Phoenix-ramDeterministic-v0', 'Phoenix-ramNoFrameskip-v0', 'Phoenix-ram-v4', 'Phoenix-ramDeterministic-v4', 'Phoenix-ramNoFrameskip-v4', 'Pitfall-v0', 'PitfallDeterministic-v0', 'PitfallNoFrameskip-v0', 'Pitfall-v4', 'PitfallDeterministic-v4', 'PitfallNoFrameskip-v4', 'Pitfall-ram-v0', 'Pitfall-ramDeterministic-v0', 'Pitfall-ramNoFrameskip-v0', 'Pitfall-ram-v4', 'Pitfall-ramDeterministic-v4', 'Pitfall-ramNoFrameskip-v4', 'Pong-v0', 'PongDeterministic-v0', 'PongNoFrameskip-v0', 'Pong-v4', 'PongDeterministic-v4', 'PongNoFrameskip-v4', 'Pong-ram-v0', 'Pong-ramDeterministic-v0', 'Pong-ramNoFrameskip-v0', 'Pong-ram-v4', 'Pong-ramDeterministic-v4', 'Pong-ramNoFrameskip-v4', 'Pooyan-v0', 'PooyanDeterministic-v0', 'PooyanNoFrameskip-v0', 'Pooyan-v4', 'PooyanDeterministic-v4', 'PooyanNoFrameskip-v4', 'Pooyan-ram-v0', 'Pooyan-ramDeterministic-v0', 'Pooyan-ramNoFrameskip-v0', 'Pooyan-ram-v4', 'Pooyan-ramDeterministic-v4', 'Pooyan-ramNoFrameskip-v4', 'PrivateEye-v0', 'PrivateEyeDeterministic-v0', 'PrivateEyeNoFrameskip-v0', 'PrivateEye-v4', 'PrivateEyeDeterministic-v4', 'PrivateEyeNoFrameskip-v4', 'PrivateEye-ram-v0', 'PrivateEye-ramDeterministic-v0', 'PrivateEye-ramNoFrameskip-v0', 'PrivateEye-ram-v4', 'PrivateEye-ramDeterministic-v4', 'PrivateEye-ramNoFrameskip-v4', 'Qbert-v0', 'QbertDeterministic-v0', 'QbertNoFrameskip-v0', 'Qbert-v4', 'QbertDeterministic-v4', 'QbertNoFrameskip-v4', 'Qbert-ram-v0', 'Qbert-ramDeterministic-v0', 'Qbert-ramNoFrameskip-v0', 'Qbert-ram-v4', 'Qbert-ramDeterministic-v4', 'Qbert-ramNoFrameskip-v4', 'Riverraid-v0', 'RiverraidDeterministic-v0', 'RiverraidNoFrameskip-v0', 'Riverraid-v4', 'RiverraidDeterministic-v4', 'RiverraidNoFrameskip-v4', 'Riverraid-ram-v0', 'Riverraid-ramDeterministic-v0', 'Riverraid-ramNoFrameskip-v0', 'Riverraid-ram-v4', 'Riverraid-ramDeterministic-v4', 'Riverraid-ramNoFrameskip-v4', 'RoadRunner-v0', 'RoadRunnerDeterministic-v0', 'RoadRunnerNoFrameskip-v0', 'RoadRunner-v4', 'RoadRunnerDeterministic-v4', 'RoadRunnerNoFrameskip-v4', 'RoadRunner-ram-v0', 'RoadRunner-ramDeterministic-v0', 'RoadRunner-ramNoFrameskip-v0', 'RoadRunner-ram-v4', 'RoadRunner-ramDeterministic-v4', 'RoadRunner-ramNoFrameskip-v4', 'Robotank-v0', 'RobotankDeterministic-v0', 'RobotankNoFrameskip-v0', 'Robotank-v4', 'RobotankDeterministic-v4', 'RobotankNoFrameskip-v4', 'Robotank-ram-v0', 'Robotank-ramDeterministic-v0', 'Robotank-ramNoFrameskip-v0', 'Robotank-ram-v4', 'Robotank-ramDeterministic-v4', 'Robotank-ramNoFrameskip-v4', 'Seaquest-v0', 'SeaquestDeterministic-v0', 'SeaquestNoFrameskip-v0', 'Seaquest-v4', 'SeaquestDeterministic-v4', 'SeaquestNoFrameskip-v4', 'Seaquest-ram-v0', 'Seaquest-ramDeterministic-v0', 'Seaquest-ramNoFrameskip-v0', 'Seaquest-ram-v4', 'Seaquest-ramDeterministic-v4', 'Seaquest-ramNoFrameskip-v4', 'Skiing-v0', 'SkiingDeterministic-v0', 'SkiingNoFrameskip-v0', 'Skiing-v4', 'SkiingDeterministic-v4', 'SkiingNoFrameskip-v4', 'Skiing-ram-v0', 'Skiing-ramDeterministic-v0', 'Skiing-ramNoFrameskip-v0', 'Skiing-ram-v4', 'Skiing-ramDeterministic-v4', 'Skiing-ramNoFrameskip-v4', 'Solaris-v0', 'SolarisDeterministic-v0', 'SolarisNoFrameskip-v0', 'Solaris-v4', 'SolarisDeterministic-v4', 'SolarisNoFrameskip-v4', 'Solaris-ram-v0', 'Solaris-ramDeterministic-v0', 'Solaris-ramNoFrameskip-v0', 'Solaris-ram-v4', 'Solaris-ramDeterministic-v4', 'Solaris-ramNoFrameskip-v4', 'SpaceInvaders-v0', 'SpaceInvadersDeterministic-v0', 'SpaceInvadersNoFrameskip-v0', 'SpaceInvaders-v4', 'SpaceInvadersDeterministic-v4', 'SpaceInvadersNoFrameskip-v4', 'SpaceInvaders-ram-v0', 'SpaceInvaders-ramDeterministic-v0', 'SpaceInvaders-ramNoFrameskip-v0', 'SpaceInvaders-ram-v4', 'SpaceInvaders-ramDeterministic-v4', 'SpaceInvaders-ramNoFrameskip-v4', 'StarGunner-v0', 'StarGunnerDeterministic-v0', 'StarGunnerNoFrameskip-v0', 'StarGunner-v4', 'StarGunnerDeterministic-v4', 'StarGunnerNoFrameskip-v4', 'StarGunner-ram-v0', 'StarGunner-ramDeterministic-v0', 'StarGunner-ramNoFrameskip-v0', 'StarGunner-ram-v4', 'StarGunner-ramDeterministic-v4', 'StarGunner-ramNoFrameskip-v4', 'Tennis-v0', 'TennisDeterministic-v0', 'TennisNoFrameskip-v0', 'Tennis-v4', 'TennisDeterministic-v4', 'TennisNoFrameskip-v4', 'Tennis-ram-v0', 'Tennis-ramDeterministic-v0', 'Tennis-ramNoFrameskip-v0', 'Tennis-ram-v4', 'Tennis-ramDeterministic-v4', 'Tennis-ramNoFrameskip-v4', 'TimePilot-v0', 'TimePilotDeterministic-v0', 'TimePilotNoFrameskip-v0', 'TimePilot-v4', 'TimePilotDeterministic-v4', 'TimePilotNoFrameskip-v4', 'TimePilot-ram-v0', 'TimePilot-ramDeterministic-v0', 'TimePilot-ramNoFrameskip-v0', 'TimePilot-ram-v4', 'TimePilot-ramDeterministic-v4', 'TimePilot-ramNoFrameskip-v4', 'Tutankham-v0', 'TutankhamDeterministic-v0', 'TutankhamNoFrameskip-v0', 'Tutankham-v4', 'TutankhamDeterministic-v4', 'TutankhamNoFrameskip-v4', 'Tutankham-ram-v0', 'Tutankham-ramDeterministic-v0', 'Tutankham-ramNoFrameskip-v0', 'Tutankham-ram-v4', 'Tutankham-ramDeterministic-v4', 'Tutankham-ramNoFrameskip-v4', 'UpNDown-v0', 'UpNDownDeterministic-v0', 'UpNDownNoFrameskip-v0', 'UpNDown-v4', 'UpNDownDeterministic-v4', 'UpNDownNoFrameskip-v4', 'UpNDown-ram-v0', 'UpNDown-ramDeterministic-v0', 'UpNDown-ramNoFrameskip-v0', 'UpNDown-ram-v4', 'UpNDown-ramDeterministic-v4', 'UpNDown-ramNoFrameskip-v4', 'Venture-v0', 'VentureDeterministic-v0', 'VentureNoFrameskip-v0', 'Venture-v4', 'VentureDeterministic-v4', 'VentureNoFrameskip-v4', 'Venture-ram-v0', 'Venture-ramDeterministic-v0', 'Venture-ramNoFrameskip-v0', 'Venture-ram-v4', 'Venture-ramDeterministic-v4', 'Venture-ramNoFrameskip-v4', 'VideoPinball-v0', 'VideoPinballDeterministic-v0', 'VideoPinballNoFrameskip-v0', 'VideoPinball-v4', 'VideoPinballDeterministic-v4', 'VideoPinballNoFrameskip-v4', 'VideoPinball-ram-v0', 'VideoPinball-ramDeterministic-v0', 'VideoPinball-ramNoFrameskip-v0', 'VideoPinball-ram-v4', 'VideoPinball-ramDeterministic-v4', 'VideoPinball-ramNoFrameskip-v4', 'WizardOfWor-v0', 'WizardOfWorDeterministic-v0', 'WizardOfWorNoFrameskip-v0', 'WizardOfWor-v4', 'WizardOfWorDeterministic-v4', 'WizardOfWorNoFrameskip-v4', 'WizardOfWor-ram-v0', 'WizardOfWor-ramDeterministic-v0', 'WizardOfWor-ramNoFrameskip-v0', 'WizardOfWor-ram-v4', 'WizardOfWor-ramDeterministic-v4', 'WizardOfWor-ramNoFrameskip-v4', 'YarsRevenge-v0', 'YarsRevengeDeterministic-v0', 'YarsRevengeNoFrameskip-v0', 'YarsRevenge-v4', 'YarsRevengeDeterministic-v4', 'YarsRevengeNoFrameskip-v4', 'YarsRevenge-ram-v0', 'YarsRevenge-ramDeterministic-v0', 'YarsRevenge-ramNoFrameskip-v0', 'YarsRevenge-ram-v4', 'YarsRevenge-ramDeterministic-v4', 'YarsRevenge-ramNoFrameskip-v4', 'Zaxxon-v0', 'ZaxxonDeterministic-v0', 'ZaxxonNoFrameskip-v0', 'Zaxxon-v4', 'ZaxxonDeterministic-v4', 'ZaxxonNoFrameskip-v4', 'Zaxxon-ram-v0', 'Zaxxon-ramDeterministic-v0', 'Zaxxon-ramNoFrameskip-v0', 'Zaxxon-ram-v4', 'Zaxxon-ramDeterministic-v4', 'Zaxxon-ramNoFrameskip-v4', 'ALE/Adventure-v5', 'ALE/Adventure-ram-v5', 'ALE/AirRaid-v5', 'ALE/AirRaid-ram-v5', 'ALE/Alien-v5', 'ALE/Alien-ram-v5', 'ALE/Amidar-v5', 'ALE/Amidar-ram-v5', 'ALE/Assault-v5', 'ALE/Assault-ram-v5', 'ALE/Asterix-v5', 'ALE/Asterix-ram-v5', 'ALE/Asteroids-v5', 'ALE/Asteroids-ram-v5', 'ALE/Atlantis2-v5', 'ALE/Atlantis2-ram-v5', 'ALE/Atlantis-v5', 'ALE/Atlantis-ram-v5', 'ALE/Backgammon-v5', 'ALE/Backgammon-ram-v5', 'ALE/BankHeist-v5', 'ALE/BankHeist-ram-v5', 'ALE/BasicMath-v5', 'ALE/BasicMath-ram-v5', 'ALE/BattleZone-v5', 'ALE/BattleZone-ram-v5', 'ALE/BeamRider-v5', 'ALE/BeamRider-ram-v5', 'ALE/Berzerk-v5', 'ALE/Berzerk-ram-v5', 'ALE/Blackjack-v5', 'ALE/Blackjack-ram-v5', 'ALE/Bowling-v5', 'ALE/Bowling-ram-v5', 'ALE/Boxing-v5', 'ALE/Boxing-ram-v5', 'ALE/Breakout-v5', 'ALE/Breakout-ram-v5', 'ALE/Carnival-v5', 'ALE/Carnival-ram-v5', 'ALE/Casino-v5', 'ALE/Casino-ram-v5', 'ALE/Centipede-v5', 'ALE/Centipede-ram-v5', 'ALE/ChopperCommand-v5', 'ALE/ChopperCommand-ram-v5', 'ALE/Combat-v5', 'ALE/Combat-ram-v5', 'ALE/CrazyClimber-v5', 'ALE/CrazyClimber-ram-v5', 'ALE/Crossbow-v5', 'ALE/Crossbow-ram-v5', 'ALE/Darkchambers-v5', 'ALE/Darkchambers-ram-v5', 'ALE/Defender-v5', 'ALE/Defender-ram-v5', 'ALE/DemonAttack-v5', 'ALE/DemonAttack-ram-v5', 'ALE/DonkeyKong-v5', 'ALE/DonkeyKong-ram-v5', 'ALE/DoubleDunk-v5', 'ALE/DoubleDunk-ram-v5', 'ALE/Earthworld-v5', 'ALE/Earthworld-ram-v5', 'ALE/ElevatorAction-v5', 'ALE/ElevatorAction-ram-v5', 'ALE/Enduro-v5', 'ALE/Enduro-ram-v5', 'ALE/Entombed-v5', 'ALE/Entombed-ram-v5', 'ALE/Et-v5', 'ALE/Et-ram-v5', 'ALE/FishingDerby-v5', 'ALE/FishingDerby-ram-v5', 'ALE/FlagCapture-v5', 'ALE/FlagCapture-ram-v5', 'ALE/Freeway-v5', 'ALE/Freeway-ram-v5', 'ALE/Frogger-v5', 'ALE/Frogger-ram-v5', 'ALE/Frostbite-v5', 'ALE/Frostbite-ram-v5', 'ALE/Galaxian-v5', 'ALE/Galaxian-ram-v5', 'ALE/Gopher-v5', 'ALE/Gopher-ram-v5', 'ALE/Gravitar-v5', 'ALE/Gravitar-ram-v5', 'ALE/Hangman-v5', 'ALE/Hangman-ram-v5', 'ALE/HauntedHouse-v5', 'ALE/HauntedHouse-ram-v5', 'ALE/Hero-v5', 'ALE/Hero-ram-v5', 'ALE/HumanCannonball-v5', 'ALE/HumanCannonball-ram-v5', 'ALE/IceHockey-v5', 'ALE/IceHockey-ram-v5', 'ALE/Jamesbond-v5', 'ALE/Jamesbond-ram-v5', 'ALE/JourneyEscape-v5', 'ALE/JourneyEscape-ram-v5', 'ALE/Joust-v5', 'ALE/Joust-ram-v5', 'ALE/Kaboom-v5', 'ALE/Kaboom-ram-v5', 'ALE/Kangaroo-v5', 'ALE/Kangaroo-ram-v5', 'ALE/KeystoneKapers-v5', 'ALE/KeystoneKapers-ram-v5', 'ALE/KingKong-v5', 'ALE/KingKong-ram-v5', 'ALE/Klax-v5', 'ALE/Klax-ram-v5', 'ALE/Koolaid-v5', 'ALE/Koolaid-ram-v5', 'ALE/Krull-v5', 'ALE/Krull-ram-v5', 'ALE/KungFuMaster-v5', 'ALE/KungFuMaster-ram-v5', 'ALE/LaserGates-v5', 'ALE/LaserGates-ram-v5', 'ALE/LostLuggage-v5', 'ALE/LostLuggage-ram-v5', 'ALE/MarioBros-v5', 'ALE/MarioBros-ram-v5', 'ALE/MazeCraze-v5', 'ALE/MazeCraze-ram-v5', 'ALE/MiniatureGolf-v5', 'ALE/MiniatureGolf-ram-v5', 'ALE/MontezumaRevenge-v5', 'ALE/MontezumaRevenge-ram-v5', 'ALE/MrDo-v5', 'ALE/MrDo-ram-v5', 'ALE/MsPacman-v5', 'ALE/MsPacman-ram-v5', 'ALE/NameThisGame-v5', 'ALE/NameThisGame-ram-v5', 'ALE/Othello-v5', 'ALE/Othello-ram-v5', 'ALE/Pacman-v5', 'ALE/Pacman-ram-v5', 'ALE/Phoenix-v5', 'ALE/Phoenix-ram-v5', 'ALE/Pitfall2-v5', 'ALE/Pitfall2-ram-v5', 'ALE/Pitfall-v5', 'ALE/Pitfall-ram-v5', 'ALE/Pong-v5', 'ALE/Pong-ram-v5', 'ALE/Pooyan-v5', 'ALE/Pooyan-ram-v5', 'ALE/PrivateEye-v5', 'ALE/PrivateEye-ram-v5', 'ALE/Qbert-v5', 'ALE/Qbert-ram-v5', 'ALE/Riverraid-v5', 'ALE/Riverraid-ram-v5', 'ALE/RoadRunner-v5', 'ALE/RoadRunner-ram-v5', 'ALE/Robotank-v5', 'ALE/Robotank-ram-v5', 'ALE/Seaquest-v5', 'ALE/Seaquest-ram-v5', 'ALE/SirLancelot-v5', 'ALE/SirLancelot-ram-v5', 'ALE/Skiing-v5', 'ALE/Skiing-ram-v5', 'ALE/Solaris-v5', 'ALE/Solaris-ram-v5', 'ALE/SpaceInvaders-v5', 'ALE/SpaceInvaders-ram-v5', 'ALE/SpaceWar-v5', 'ALE/SpaceWar-ram-v5', 'ALE/StarGunner-v5', 'ALE/StarGunner-ram-v5', 'ALE/Superman-v5', 'ALE/Superman-ram-v5', 'ALE/Surround-v5', 'ALE/Surround-ram-v5', 'ALE/Tennis-v5', 'ALE/Tennis-ram-v5', 'ALE/Tetris-v5', 'ALE/Tetris-ram-v5', 'ALE/TicTacToe3D-v5', 'ALE/TicTacToe3D-ram-v5', 'ALE/TimePilot-v5', 'ALE/TimePilot-ram-v5', 'ALE/Trondead-v5', 'ALE/Trondead-ram-v5', 'ALE/Turmoil-v5', 'ALE/Turmoil-ram-v5', 'ALE/Tutankham-v5', 'ALE/Tutankham-ram-v5', 'ALE/UpNDown-v5', 'ALE/UpNDown-ram-v5', 'ALE/Venture-v5', 'ALE/Venture-ram-v5', 'ALE/VideoCheckers-v5', 'ALE/VideoCheckers-ram-v5', 'ALE/VideoChess-v5', 'ALE/VideoChess-ram-v5', 'ALE/VideoCube-v5', 'ALE/VideoCube-ram-v5', 'ALE/VideoPinball-v5', 'ALE/VideoPinball-ram-v5', 'ALE/Warlords-v5', 'ALE/Warlords-ram-v5', 'ALE/WizardOfWor-v5', 'ALE/WizardOfWor-ram-v5', 'ALE/WordZapper-v5', 'ALE/WordZapper-ram-v5', 'ALE/YarsRevenge-v5', 'ALE/YarsRevenge-ram-v5', 'ALE/Zaxxon-v5', 'ALE/Zaxxon-ram-v5'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JG5_P7kKezZ",
        "outputId": "5824594a-086d-4b80-ef69-a2b00ad07b7b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium[accept-rom-license, atari]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHxx_AlLLUnd",
        "outputId": "39a29735-69a6-48b2-a7b2-cf40654755d6"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium import spaces"
      ],
      "metadata": {
        "id": "MjKiDNh3M2B5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n"
      ],
      "metadata": {
        "id": "7Lp8--lxM9Hq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envs = [InputWrapper(gym.make(name)) for name in ('ALE/Breakout-v5', 'ALE/AirRaid-v5', 'ALE/Pong-v5')]"
      ],
      "metadata": {
        "id": "mT5SBC6kMt2M"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = envs[0].observation_space.shape\n",
        "net_discr = Discriminator(input_shape = input_shape).to(device)\n",
        "net_gener = Generator(output_shape = input_shape).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "RLMPzpHMNVvd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "REPORT_EVERY_ITER = 100\n",
        "SAVE_IMAGE_EVERY_ITER = 1000\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "X8k-AuHDN7zI"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objective = nn.BCELoss()\n",
        "gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "XpErLDoZNuZT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = torch.ones(BATCH_SIZE, device=device)\n",
        "false_labels= torch.ones(BATCH_SIZE,device=device)\n"
      ],
      "metadata": {
        "id": "paEy6JY4MzUY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " writer = SummaryWriter()\n",
        " gen_losses = []\n",
        " disc_losses = []\n",
        ""
      ],
      "metadata": {
        "id": "7iwa3O8EPVCV"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "import torchvision.utils as vutils\n",
        "# Get the root logger of Gymnasium\n",
        "gym_logger = logging.getLogger('gymnasium')\n",
        "\n",
        "# Set the desired log level (e.g., INFO, DEBUG, WARNING, ERROR, CRITICAL)\n",
        "gym_logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "ByCiid0BZ3pr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_no = 0\n",
        "ts_start = time.time()\n",
        "for batch in iterate_batches(envs, BATCH_SIZE):\n",
        "  gen_input = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1,1)\n",
        "  gen_input.normal_(0,1)\n",
        "  gen_input = gen_input.to(device)\n",
        "  batch = batch.to(device)\n",
        "  generator_output = net_gener(gen_input)\n",
        "  dis_optimizer.zero_grad()\n",
        "  dis_output_true = net_discr(batch)\n",
        "  dis_output_false = net_discr(generator_output.detach())\n",
        "  dis_loss = objective(dis_output_true, true_labels) + objective(dis_output_false, false_labels)\n",
        "  dis_loss.backward()\n",
        "  dis_optimizer.step()\n",
        "  disc_losses.append(dis_loss.item())\n",
        "\n",
        "  gen_optimizer.zero_grad()\n",
        "  dis_output = net_discr(generator_output)\n",
        "  gen_loss = objective(dis_output, true_labels)\n",
        "  gen_loss.backward()\n",
        "  gen_optimizer.step()\n",
        "  gen_losses.append(gen_loss.item())\n",
        "\n",
        "  iter_no += 1\n",
        "  if iter_no % REPORT_EVERY_ITER == 0:\n",
        "    dt = time.time() - ts_start\n",
        "    ts_start = time.time()\n",
        "    gym_logger.info(\"Iter %d in %.2fs: gen_loss=%.3e, dis_loss=%.3e\",\n",
        "                     iter_no, dt, np.mean(gen_losses), np.mean(disc_losses))\n",
        "    writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no)\n",
        "    writer.add_scalar(\"dis_loss\", np.mean(disc_losses), iter_no)\n",
        "    gen_losses = []\n",
        "    disc_losses = []\n",
        "    if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
        "       img = vutils.make_grid(generator_output.data[:64], normalize=True)\n",
        "       writer.add_image(\"fake\", img, iter_no)\n",
        "       img = vutils.make_grid(batch.data[:64], normalize=True)\n",
        "       writer.add_image(\"real\", img, iter_no)\n",
        "\n",
        "  if iter_no >= 20000:\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9n0puWePdMC",
        "outputId": "f47b8ac4-6758-46a4-af61-67d72a736788"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gymnasium:Iter 100 in 14.53s: gen_loss=1.974e-03, dis_loss=1.676e-02\n",
            "INFO:gymnasium:Iter 200 in 8.47s: gen_loss=4.776e-05, dis_loss=1.907e-04\n",
            "INFO:gymnasium:Iter 300 in 4.50s: gen_loss=1.902e-05, dis_loss=6.179e-05\n",
            "INFO:gymnasium:Iter 400 in 3.66s: gen_loss=1.119e-05, dis_loss=3.657e-05\n",
            "INFO:gymnasium:Iter 500 in 3.65s: gen_loss=7.471e-06, dis_loss=2.698e-05\n",
            "INFO:gymnasium:Iter 600 in 3.35s: gen_loss=5.444e-06, dis_loss=1.879e-05\n",
            "INFO:gymnasium:Iter 700 in 3.34s: gen_loss=4.124e-06, dis_loss=1.587e-05\n",
            "INFO:gymnasium:Iter 800 in 3.84s: gen_loss=3.279e-06, dis_loss=1.042e-05\n",
            "INFO:gymnasium:Iter 900 in 3.61s: gen_loss=2.697e-06, dis_loss=1.073e-05\n",
            "INFO:gymnasium:Iter 1000 in 3.35s: gen_loss=2.210e-06, dis_loss=9.186e-06\n",
            "INFO:gymnasium:Iter 1100 in 3.42s: gen_loss=1.861e-06, dis_loss=7.985e-06\n",
            "INFO:gymnasium:Iter 1200 in 3.96s: gen_loss=1.587e-06, dis_loss=5.805e-06\n",
            "INFO:gymnasium:Iter 1300 in 3.36s: gen_loss=1.375e-06, dis_loss=4.938e-06\n",
            "INFO:gymnasium:Iter 1400 in 3.34s: gen_loss=1.202e-06, dis_loss=5.203e-06\n",
            "INFO:gymnasium:Iter 1500 in 3.46s: gen_loss=1.062e-06, dis_loss=3.921e-06\n",
            "INFO:gymnasium:Iter 1600 in 3.84s: gen_loss=9.464e-07, dis_loss=3.371e-06\n",
            "INFO:gymnasium:Iter 1700 in 3.34s: gen_loss=8.468e-07, dis_loss=3.053e-06\n",
            "INFO:gymnasium:Iter 1800 in 3.49s: gen_loss=7.599e-07, dis_loss=3.143e-06\n",
            "INFO:gymnasium:Iter 1900 in 4.26s: gen_loss=6.891e-07, dis_loss=2.457e-06\n",
            "INFO:gymnasium:Iter 2000 in 3.44s: gen_loss=6.207e-07, dis_loss=2.475e-06\n",
            "INFO:gymnasium:Iter 2100 in 3.38s: gen_loss=5.623e-07, dis_loss=2.160e-06\n",
            "INFO:gymnasium:Iter 2200 in 3.31s: gen_loss=5.195e-07, dis_loss=1.941e-06\n",
            "INFO:gymnasium:Iter 2300 in 3.97s: gen_loss=4.735e-07, dis_loss=1.711e-06\n",
            "INFO:gymnasium:Iter 2400 in 3.34s: gen_loss=4.395e-07, dis_loss=1.829e-06\n",
            "INFO:gymnasium:Iter 2500 in 3.32s: gen_loss=3.988e-07, dis_loss=1.490e-06\n",
            "INFO:gymnasium:Iter 2600 in 4.14s: gen_loss=3.734e-07, dis_loss=1.367e-06\n",
            "INFO:gymnasium:Iter 2700 in 3.71s: gen_loss=3.407e-07, dis_loss=1.458e-06\n",
            "INFO:gymnasium:Iter 2800 in 3.33s: gen_loss=3.161e-07, dis_loss=1.292e-06\n",
            "INFO:gymnasium:Iter 2900 in 3.55s: gen_loss=2.963e-07, dis_loss=1.187e-06\n",
            "INFO:gymnasium:Iter 3000 in 3.86s: gen_loss=2.760e-07, dis_loss=1.033e-06\n",
            "INFO:gymnasium:Iter 3100 in 3.53s: gen_loss=2.577e-07, dis_loss=9.455e-07\n",
            "INFO:gymnasium:Iter 3200 in 3.38s: gen_loss=2.394e-07, dis_loss=8.594e-07\n",
            "INFO:gymnasium:Iter 3300 in 3.37s: gen_loss=2.237e-07, dis_loss=8.345e-07\n",
            "INFO:gymnasium:Iter 3400 in 3.92s: gen_loss=2.087e-07, dis_loss=9.745e-07\n",
            "INFO:gymnasium:Iter 3500 in 3.34s: gen_loss=1.901e-07, dis_loss=8.476e-07\n",
            "INFO:gymnasium:Iter 3600 in 3.33s: gen_loss=1.774e-07, dis_loss=7.309e-07\n",
            "INFO:gymnasium:Iter 3700 in 3.51s: gen_loss=1.662e-07, dis_loss=7.311e-07\n",
            "INFO:gymnasium:Iter 3800 in 3.85s: gen_loss=1.545e-07, dis_loss=5.977e-07\n",
            "INFO:gymnasium:Iter 3900 in 3.35s: gen_loss=1.451e-07, dis_loss=6.032e-07\n",
            "INFO:gymnasium:Iter 4000 in 3.34s: gen_loss=1.368e-07, dis_loss=5.299e-07\n",
            "INFO:gymnasium:Iter 4100 in 3.80s: gen_loss=1.325e-07, dis_loss=5.621e-07\n",
            "INFO:gymnasium:Iter 4200 in 3.49s: gen_loss=1.254e-07, dis_loss=5.124e-07\n",
            "INFO:gymnasium:Iter 4300 in 3.80s: gen_loss=1.236e-07, dis_loss=4.645e-07\n",
            "INFO:gymnasium:Iter 4400 in 3.33s: gen_loss=1.212e-07, dis_loss=5.114e-07\n",
            "INFO:gymnasium:Iter 4500 in 3.91s: gen_loss=1.194e-07, dis_loss=4.649e-07\n",
            "INFO:gymnasium:Iter 4600 in 3.36s: gen_loss=1.170e-07, dis_loss=3.937e-07\n",
            "INFO:gymnasium:Iter 4700 in 3.35s: gen_loss=1.151e-07, dis_loss=3.828e-07\n",
            "INFO:gymnasium:Iter 4800 in 3.57s: gen_loss=1.101e-07, dis_loss=3.988e-07\n",
            "INFO:gymnasium:Iter 4900 in 3.68s: gen_loss=1.062e-07, dis_loss=3.520e-07\n",
            "INFO:gymnasium:Iter 5000 in 3.34s: gen_loss=9.671e-08, dis_loss=4.323e-07\n",
            "INFO:gymnasium:Iter 5100 in 3.40s: gen_loss=8.605e-08, dis_loss=3.156e-07\n",
            "INFO:gymnasium:Iter 5200 in 3.84s: gen_loss=7.547e-08, dis_loss=2.922e-07\n",
            "INFO:gymnasium:Iter 5300 in 3.40s: gen_loss=6.467e-08, dis_loss=2.672e-07\n",
            "INFO:gymnasium:Iter 5400 in 3.35s: gen_loss=5.409e-08, dis_loss=2.830e-07\n",
            "INFO:gymnasium:Iter 5500 in 3.35s: gen_loss=4.597e-08, dis_loss=2.308e-07\n",
            "INFO:gymnasium:Iter 5600 in 3.86s: gen_loss=3.524e-08, dis_loss=2.506e-07\n",
            "INFO:gymnasium:Iter 5700 in 3.34s: gen_loss=2.973e-08, dis_loss=1.930e-07\n",
            "INFO:gymnasium:Iter 5800 in 3.32s: gen_loss=2.533e-08, dis_loss=1.727e-07\n",
            "INFO:gymnasium:Iter 5900 in 3.59s: gen_loss=2.176e-08, dis_loss=1.728e-07\n",
            "INFO:gymnasium:Iter 6000 in 3.61s: gen_loss=1.788e-08, dis_loss=1.667e-07\n",
            "INFO:gymnasium:Iter 6100 in 3.37s: gen_loss=1.252e-08, dis_loss=1.834e-07\n",
            "INFO:gymnasium:Iter 6200 in 3.32s: gen_loss=1.125e-08, dis_loss=1.676e-07\n",
            "INFO:gymnasium:Iter 6300 in 3.84s: gen_loss=9.984e-09, dis_loss=1.132e-07\n",
            "INFO:gymnasium:Iter 6400 in 3.36s: gen_loss=7.600e-09, dis_loss=1.290e-07\n",
            "INFO:gymnasium:Iter 6500 in 3.31s: gen_loss=5.811e-09, dis_loss=1.458e-07\n",
            "INFO:gymnasium:Iter 6600 in 3.32s: gen_loss=5.588e-09, dis_loss=1.007e-07\n",
            "INFO:gymnasium:Iter 6700 in 3.90s: gen_loss=4.023e-09, dis_loss=1.011e-07\n",
            "INFO:gymnasium:Iter 6800 in 3.32s: gen_loss=4.843e-09, dis_loss=1.185e-07\n",
            "INFO:gymnasium:Iter 6900 in 3.25s: gen_loss=2.682e-09, dis_loss=9.716e-08\n",
            "INFO:gymnasium:Iter 7000 in 3.43s: gen_loss=1.863e-09, dis_loss=9.067e-08\n",
            "INFO:gymnasium:Iter 7100 in 3.85s: gen_loss=1.788e-09, dis_loss=6.065e-08\n",
            "INFO:gymnasium:Iter 7200 in 3.27s: gen_loss=1.788e-09, dis_loss=5.320e-08\n",
            "INFO:gymnasium:Iter 7300 in 3.28s: gen_loss=1.416e-09, dis_loss=9.134e-08\n",
            "INFO:gymnasium:Iter 7400 in 3.67s: gen_loss=1.267e-09, dis_loss=8.166e-08\n",
            "INFO:gymnasium:Iter 7500 in 3.49s: gen_loss=7.451e-10, dis_loss=6.415e-08\n",
            "INFO:gymnasium:Iter 7600 in 3.28s: gen_loss=6.706e-10, dis_loss=4.947e-08\n",
            "INFO:gymnasium:Iter 7700 in 3.38s: gen_loss=1.043e-09, dis_loss=4.426e-08\n",
            "INFO:gymnasium:Iter 7800 in 3.93s: gen_loss=6.706e-10, dis_loss=6.475e-08\n",
            "INFO:gymnasium:Iter 7900 in 3.42s: gen_loss=5.960e-10, dis_loss=4.575e-08\n",
            "INFO:gymnasium:Iter 8000 in 3.86s: gen_loss=3.725e-10, dis_loss=3.104e-07\n",
            "INFO:gymnasium:Iter 8100 in 3.49s: gen_loss=7.451e-11, dis_loss=5.029e-08\n",
            "INFO:gymnasium:Iter 8200 in 3.76s: gen_loss=7.451e-11, dis_loss=2.936e-08\n",
            "INFO:gymnasium:Iter 8300 in 3.26s: gen_loss=7.451e-11, dis_loss=3.062e-08\n",
            "INFO:gymnasium:Iter 8400 in 3.27s: gen_loss=0.000e+00, dis_loss=1.848e-08\n",
            "INFO:gymnasium:Iter 8500 in 3.63s: gen_loss=1.490e-10, dis_loss=4.180e-08\n",
            "INFO:gymnasium:Iter 8600 in 3.58s: gen_loss=0.000e+00, dis_loss=2.451e-08\n",
            "INFO:gymnasium:Iter 8700 in 3.28s: gen_loss=0.000e+00, dis_loss=1.937e-08\n",
            "INFO:gymnasium:Iter 8800 in 3.29s: gen_loss=7.451e-11, dis_loss=2.354e-08\n",
            "INFO:gymnasium:Iter 8900 in 3.81s: gen_loss=7.451e-11, dis_loss=1.714e-08\n",
            "INFO:gymnasium:Iter 9000 in 3.38s: gen_loss=2.235e-10, dis_loss=1.922e-08\n",
            "INFO:gymnasium:Iter 9100 in 3.34s: gen_loss=0.000e+00, dis_loss=1.460e-08\n",
            "INFO:gymnasium:Iter 9200 in 3.24s: gen_loss=7.451e-11, dis_loss=2.488e-08\n",
            "INFO:gymnasium:Iter 9300 in 3.86s: gen_loss=0.000e+00, dis_loss=1.825e-08\n",
            "INFO:gymnasium:Iter 9400 in 3.33s: gen_loss=0.000e+00, dis_loss=1.907e-08\n",
            "INFO:gymnasium:Iter 9500 in 3.27s: gen_loss=0.000e+00, dis_loss=7.078e-09\n",
            "INFO:gymnasium:Iter 9600 in 3.44s: gen_loss=0.000e+00, dis_loss=1.341e-08\n",
            "INFO:gymnasium:Iter 9700 in 3.79s: gen_loss=0.000e+00, dis_loss=1.706e-08\n",
            "INFO:gymnasium:Iter 9800 in 3.27s: gen_loss=0.000e+00, dis_loss=1.028e-08\n",
            "INFO:gymnasium:Iter 9900 in 3.27s: gen_loss=0.000e+00, dis_loss=1.214e-08\n",
            "INFO:gymnasium:Iter 10000 in 3.55s: gen_loss=7.451e-11, dis_loss=1.080e-08\n",
            "INFO:gymnasium:Iter 10100 in 3.69s: gen_loss=0.000e+00, dis_loss=9.164e-09\n",
            "INFO:gymnasium:Iter 10200 in 3.29s: gen_loss=7.451e-11, dis_loss=1.229e-08\n",
            "INFO:gymnasium:Iter 10300 in 3.32s: gen_loss=0.000e+00, dis_loss=3.949e-09\n",
            "INFO:gymnasium:Iter 10400 in 3.80s: gen_loss=0.000e+00, dis_loss=1.162e-08\n",
            "INFO:gymnasium:Iter 10500 in 3.35s: gen_loss=0.000e+00, dis_loss=7.749e-09\n",
            "INFO:gymnasium:Iter 10600 in 3.32s: gen_loss=0.000e+00, dis_loss=6.706e-09\n",
            "INFO:gymnasium:Iter 10700 in 3.31s: gen_loss=0.000e+00, dis_loss=8.568e-09\n",
            "INFO:gymnasium:Iter 10800 in 3.96s: gen_loss=0.000e+00, dis_loss=2.608e-09\n",
            "INFO:gymnasium:Iter 10900 in 3.30s: gen_loss=0.000e+00, dis_loss=4.768e-09\n",
            "INFO:gymnasium:Iter 11000 in 3.27s: gen_loss=0.000e+00, dis_loss=5.737e-09\n",
            "INFO:gymnasium:Iter 11100 in 3.45s: gen_loss=0.000e+00, dis_loss=7.972e-09\n",
            "INFO:gymnasium:Iter 11200 in 3.91s: gen_loss=0.000e+00, dis_loss=5.364e-09\n",
            "INFO:gymnasium:Iter 11300 in 3.25s: gen_loss=0.000e+00, dis_loss=6.706e-09\n",
            "INFO:gymnasium:Iter 11400 in 3.28s: gen_loss=0.000e+00, dis_loss=2.310e-09\n",
            "INFO:gymnasium:Iter 11500 in 3.49s: gen_loss=0.000e+00, dis_loss=4.470e-09\n",
            "INFO:gymnasium:Iter 11600 in 3.76s: gen_loss=0.000e+00, dis_loss=4.321e-09\n",
            "INFO:gymnasium:Iter 11700 in 3.28s: gen_loss=0.000e+00, dis_loss=3.949e-09\n",
            "INFO:gymnasium:Iter 11800 in 3.26s: gen_loss=0.000e+00, dis_loss=3.725e-09\n",
            "INFO:gymnasium:Iter 11900 in 3.59s: gen_loss=0.000e+00, dis_loss=3.576e-09\n",
            "INFO:gymnasium:Iter 12000 in 3.59s: gen_loss=0.000e+00, dis_loss=4.396e-09\n",
            "INFO:gymnasium:Iter 12100 in 3.35s: gen_loss=0.000e+00, dis_loss=3.949e-09\n",
            "INFO:gymnasium:Iter 12200 in 3.26s: gen_loss=0.000e+00, dis_loss=1.267e-09\n",
            "INFO:gymnasium:Iter 12300 in 3.76s: gen_loss=0.000e+00, dis_loss=2.384e-09\n",
            "INFO:gymnasium:Iter 12400 in 3.50s: gen_loss=0.000e+00, dis_loss=2.235e-09\n",
            "INFO:gymnasium:Iter 12500 in 3.26s: gen_loss=0.000e+00, dis_loss=6.482e-09\n",
            "INFO:gymnasium:Iter 12600 in 3.32s: gen_loss=0.000e+00, dis_loss=1.788e-09\n",
            "INFO:gymnasium:Iter 12700 in 3.88s: gen_loss=0.000e+00, dis_loss=8.196e-10\n",
            "INFO:gymnasium:Iter 12800 in 3.32s: gen_loss=0.000e+00, dis_loss=2.906e-09\n",
            "INFO:gymnasium:Iter 12900 in 3.28s: gen_loss=0.000e+00, dis_loss=1.341e-09\n",
            "INFO:gymnasium:Iter 13000 in 3.29s: gen_loss=0.000e+00, dis_loss=1.192e-09\n",
            "INFO:gymnasium:Iter 13100 in 3.95s: gen_loss=0.000e+00, dis_loss=2.384e-09\n",
            "INFO:gymnasium:Iter 13200 in 3.28s: gen_loss=0.000e+00, dis_loss=1.937e-09\n",
            "INFO:gymnasium:Iter 13300 in 3.31s: gen_loss=0.000e+00, dis_loss=1.788e-09\n",
            "INFO:gymnasium:Iter 13400 in 3.35s: gen_loss=0.000e+00, dis_loss=2.757e-09\n",
            "INFO:gymnasium:Iter 13500 in 3.76s: gen_loss=0.000e+00, dis_loss=2.012e-09\n",
            "INFO:gymnasium:Iter 13600 in 3.27s: gen_loss=0.000e+00, dis_loss=8.196e-10\n",
            "INFO:gymnasium:Iter 13700 in 3.26s: gen_loss=0.000e+00, dis_loss=4.470e-10\n",
            "INFO:gymnasium:Iter 13800 in 3.57s: gen_loss=0.000e+00, dis_loss=9.686e-10\n",
            "INFO:gymnasium:Iter 13900 in 3.66s: gen_loss=0.000e+00, dis_loss=1.118e-09\n",
            "INFO:gymnasium:Iter 14000 in 3.31s: gen_loss=0.000e+00, dis_loss=5.215e-10\n",
            "INFO:gymnasium:Iter 14100 in 3.32s: gen_loss=0.000e+00, dis_loss=7.451e-10\n",
            "INFO:gymnasium:Iter 14200 in 4.41s: gen_loss=0.000e+00, dis_loss=3.949e-09\n",
            "INFO:gymnasium:Iter 14300 in 3.56s: gen_loss=0.000e+00, dis_loss=6.706e-10\n",
            "INFO:gymnasium:Iter 14400 in 3.25s: gen_loss=0.000e+00, dis_loss=3.725e-10\n",
            "INFO:gymnasium:Iter 14500 in 3.25s: gen_loss=0.000e+00, dis_loss=8.196e-10\n",
            "INFO:gymnasium:Iter 14600 in 3.84s: gen_loss=0.000e+00, dis_loss=2.235e-09\n",
            "INFO:gymnasium:Iter 14700 in 3.38s: gen_loss=0.000e+00, dis_loss=7.451e-10\n",
            "INFO:gymnasium:Iter 14800 in 3.26s: gen_loss=0.000e+00, dis_loss=1.490e-10\n",
            "INFO:gymnasium:Iter 14900 in 3.25s: gen_loss=0.000e+00, dis_loss=6.706e-10\n",
            "INFO:gymnasium:Iter 15000 in 4.03s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 15100 in 3.33s: gen_loss=0.000e+00, dis_loss=1.341e-09\n",
            "INFO:gymnasium:Iter 15200 in 3.25s: gen_loss=0.000e+00, dis_loss=8.196e-10\n",
            "INFO:gymnasium:Iter 15300 in 3.27s: gen_loss=0.000e+00, dis_loss=8.941e-10\n",
            "INFO:gymnasium:Iter 15400 in 3.88s: gen_loss=0.000e+00, dis_loss=9.686e-10\n",
            "INFO:gymnasium:Iter 15500 in 3.26s: gen_loss=0.000e+00, dis_loss=5.960e-10\n",
            "INFO:gymnasium:Iter 15600 in 3.32s: gen_loss=0.000e+00, dis_loss=2.235e-10\n",
            "INFO:gymnasium:Iter 15700 in 3.45s: gen_loss=0.000e+00, dis_loss=2.980e-10\n",
            "INFO:gymnasium:Iter 15800 in 3.67s: gen_loss=0.000e+00, dis_loss=2.235e-10\n",
            "INFO:gymnasium:Iter 15900 in 3.29s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 16000 in 3.25s: gen_loss=0.000e+00, dis_loss=2.459e-09\n",
            "INFO:gymnasium:Iter 16100 in 3.79s: gen_loss=0.000e+00, dis_loss=5.960e-10\n",
            "INFO:gymnasium:Iter 16200 in 3.44s: gen_loss=0.000e+00, dis_loss=7.451e-10\n",
            "INFO:gymnasium:Iter 16300 in 3.26s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 16400 in 3.31s: gen_loss=0.000e+00, dis_loss=1.490e-10\n",
            "INFO:gymnasium:Iter 16500 in 3.88s: gen_loss=0.000e+00, dis_loss=8.196e-10\n",
            "INFO:gymnasium:Iter 16600 in 3.33s: gen_loss=0.000e+00, dis_loss=2.556e-08\n",
            "INFO:gymnasium:Iter 16700 in 3.24s: gen_loss=0.000e+00, dis_loss=2.235e-10\n",
            "INFO:gymnasium:Iter 16800 in 3.28s: gen_loss=0.000e+00, dis_loss=3.129e-09\n",
            "INFO:gymnasium:Iter 16900 in 3.92s: gen_loss=0.000e+00, dis_loss=7.451e-10\n",
            "INFO:gymnasium:Iter 17000 in 3.26s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 17100 in 3.34s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 17200 in 3.37s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 17300 in 3.87s: gen_loss=0.000e+00, dis_loss=2.235e-10\n",
            "INFO:gymnasium:Iter 17400 in 3.27s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 17500 in 3.27s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 17600 in 3.48s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 17700 in 3.77s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 17800 in 3.25s: gen_loss=0.000e+00, dis_loss=3.725e-10\n",
            "INFO:gymnasium:Iter 17900 in 3.28s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 18000 in 3.59s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 18100 in 3.74s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 18200 in 3.27s: gen_loss=0.000e+00, dis_loss=1.043e-09\n",
            "INFO:gymnasium:Iter 18300 in 3.28s: gen_loss=0.000e+00, dis_loss=7.451e-11\n",
            "INFO:gymnasium:Iter 18400 in 3.74s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 18500 in 3.38s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 18600 in 3.28s: gen_loss=0.000e+00, dis_loss=7.451e-11\n",
            "INFO:gymnasium:Iter 18700 in 3.26s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 18800 in 3.90s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 18900 in 3.27s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19000 in 3.25s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19100 in 3.41s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19200 in 3.86s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19300 in 3.29s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19400 in 3.30s: gen_loss=0.000e+00, dis_loss=2.980e-10\n",
            "INFO:gymnasium:Iter 19500 in 3.51s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19600 in 3.60s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19700 in 3.34s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19800 in 3.27s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 19900 in 3.83s: gen_loss=0.000e+00, dis_loss=0.000e+00\n",
            "INFO:gymnasium:Iter 20000 in 3.47s: gen_loss=0.000e+00, dis_loss=0.000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some mundane deep learning tasks, pytorch might be too low level. we may be benefitted by a high level wrapper. There are several libraries which offer open source , faster implementations of common pytorch tasks like ptlearn, fastai and ignite."
      ],
      "metadata": {
        "id": "uttUNOR8NtM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignite simplifies writing training loops in pytorch among other task simplifications"
      ],
      "metadata": {
        "id": "QSbAIPQnOYZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-ignite\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtWw9OCYPvbg",
        "outputId": "3948faec-cdf0-4f19-f7fc-7b4d2b8becbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.5.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (2.5.1+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=1.3->pytorch-ignite)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (3.0.2)\n",
            "Downloading pytorch_ignite-0.5.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.7/312.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-ignite\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-ignite-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GAN training on Atari using Ignite\n",
        "\n",
        "#includes\n",
        "import cv2\n",
        "import random\n",
        "import torchvision.utils as vutils\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import Timer\n",
        "from ignite.metrics import RunningAverage\n",
        "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdPncSLLOe8J",
        "outputId": "45a72855-b882-4269-d05e-704b19836741"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LATENT_VECTOR_SIZE = 100\n",
        "DISCR_FILTERS = 64\n",
        "GENER_FILTERS = 64\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "6ju_rvDCO5-t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "REPORT_EVERY_ITER = 100\n",
        "SAVE_IMAGE_EVERY_ITER = 1000"
      ],
      "metadata": {
        "id": "20EumnBFPpnL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(trainer, batch):\n",
        "  gen_input = torch.FloatTensor(BATCH_SIZE,LATENT_SIZE,1,1)\n",
        "  gen_input.normal_(0,1)\n",
        "  gen_input = gen_input.to(device)\n",
        "  batch = batcn.to(device)\n",
        "  gen_output = net_gener(gen_input)\n",
        "\n",
        "  dis_optimizer.zero_grad()\n",
        "  dis_output_true = net_discr(batch)\n",
        "  dis_output_false = net_discr(gen_output.detach())\n",
        "  dis_loss = objective(dis_output_true, true_labels) + objective(dis_output_false, false_labels)\n",
        "  dis_loss.backward()\n",
        "  dis_optimizer.step()\n",
        "\n",
        "  gen_optimizer.zero_grad()\n",
        "  dis_output = net_discr(gen_output)\n",
        "  gen_loss = objective(dis_output, true_labels)\n",
        "  gen_loss.backward()\n",
        "  gen_optimizer.step()\n",
        "\n",
        "  if trainer.state.iteration % SAVE_IMAGE_EVERY_ITER == 0:\n",
        "            fake_img = vutils.make_grid(gen_output_v.data[:64], normalize=True)\n",
        "            trainer.tb.writer.add_image(\"fake\", fake_img, trainer.state.iteration)\n",
        "            real_img = vutils.make_grid(batch_v.data[:64], normalize=True)\n",
        "            trainer.tb.writer.add_image(\"real\", real_img, trainer.state.iteration)\n",
        "            trainer.tb.writer.flush()\n",
        "  return dis_loss.item(), gen_loss.item()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VLPwQ05KPzIS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LATENT_VECTOR_SIZE = 100\n",
        "DISCR_FILTERS = 64\n",
        "GENER_FILTERS = 64\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# dimension input image will be rescaled\n",
        "IMAGE_SIZE = 64\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "REPORT_EVERY_ITER = 100\n",
        "SAVE_IMAGE_EVERY_ITER = 1000"
      ],
      "metadata": {
        "id": "pXF5f0e5ZMSx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = Engine(process_batch) # ignite engine\n",
        "tb = tb_logger.TensorboardLogger(log_dir=None)\n",
        "engine.tb = tb\n",
        "RunningAverage(output_transform=lambda x: x[0]).attach(engine,\"dis_loss\")\n",
        "RunningAverage(output_transform = lambda x : x[1]).attach(engine,\"gen_loss\")\n",
        "\n",
        "handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['gen_loss', 'dis_loss'])\n",
        "tb.attach(engine, log_handler=handler, event_name=Events.ITERATION_COMPLETED)\n",
        "timer = Timer()\n",
        "timer.attach(engine)\n",
        "\n",
        "@engine.on(Events.ITERATION_COMPLETED)\n",
        "def log_losses(trainer):\n",
        "        if trainer.state.iteration % REPORT_EVERY_ITER == 0:\n",
        "            log.info(\"%d in %.2fs: gen_loss=%f, dis_loss=%f\",\n",
        "                     trainer.state.iteration, timer.value(),\n",
        "                     trainer.state.metrics['avg_loss_gen'],\n",
        "                     trainer.state.metrics['avg_loss_dis'])\n",
        "            timer.reset()"
      ],
      "metadata": {
        "id": "iX6dEc_qXFOX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine.run(data=iterate_batches(envs,BATCH_SIZE))"
      ],
      "metadata": {
        "id": "ystTcUunYe8O"
      },
      "execution_count": 32,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}