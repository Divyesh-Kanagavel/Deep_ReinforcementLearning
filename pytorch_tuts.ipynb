{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch - a DL library for manipulation of tensors [multi-dimensional arrays]. \\\n",
    "supports 13 different types - float32, float16, bfloat16(higher exponent), float64\n",
    "complex : 32,64,128 bits, int : int8, uint8, int16, int32, int64 and bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors of different types are represented by different classes - torch.FloatTensor (for float32), torch.LongTensor(int64), torch.ByteTensor(uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.FloatTensor(3,2) # calling the constructor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(3,2) # torch.FloatTensor(3,2) initializes with zeros but in the previous versions, it kept the tensor uninitialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative approach\n",
    "a = torch.FloatTensor(3,2)\n",
    "a.zero_()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of operations on tensors : inplace and functional \\\n",
    "Inplace operations have underscore appended to their name and operate on the tensor's content. The functional equivalent creates a copy. \\\n",
    "Inplace - more efficient and does not require extra memory but might lead to hidden bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor from python iterable like list, tuple\n",
    "\n",
    "a = torch.FloatTensor([[1,2],[3,4],[5,6]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2), dtype('float64'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.zeros(shape = (3,2))\n",
    "n.shape , n.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape, b.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in deep learning, float64 is too much memory overhead. float32 or float16 is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to convert from numpy array to torch tensor, torch.from_numpy was used but is now deprecated and torch.tensor() is encouraged with torch datatypes available as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2) float64\n"
     ]
    }
   ],
   "source": [
    "n = np.zeros(shape=(3,2))\n",
    "print(n.shape, n.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(n, dtype=torch.float32)\n",
    "print(t.shape, t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "#Scalar tensor - Now, zero-dimensional tensors are natively supported and returned by the appropriate functions\n",
    "a = torch.tensor([1,2,3])\n",
    "s = a.sum()\n",
    "print(s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(s.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU tensors: \n",
    "Pytorch supports CUDA GPUs. it has two versions - CPU and GPU.where to process the tensors depends on the tensor configuration. GPU tensors reside in the torch.cuda class instead of the torch package. So, the tensor is torch.cuda.FloatTensor instead of torch.FloatTensor. \\\n",
    "Under the hood, there is no CPU, GPU. there is a backend, which is an abstract computation device with memory. it could be CUDA, CPU or Apple Metal performance Shader given by mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3,4])\n",
    "print(a.shape, a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.], device='mps:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.to('mps') # tensor copied to Apple's MPS \n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = 'mps:0' refers to the fact that the computation device in use for tensor c is mps and it uses the first card. if there are multiple cards, we could have mps:1 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4., 5.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4., 5.], device='mps:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient calculation methods : \n",
    "Static graph method : define your calculations in advance and cannot be changed later. graph is optimized by the dl library like tensorflow/theano and many other DL toolkits\n",
    "Dynamic graph method : As you apply transformations on the data, the dl library will keep track of the computations and when requested will compute the gradients , accumulating the gradients of the network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From version 2.0, pytorch has torch.compile() method which speeds up pytorch code by using JIT (just in time) compiling into optimized kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
